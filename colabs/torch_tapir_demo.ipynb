{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fetSbKRrczhD"
      },
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYtT7GpuVgQ"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <h1 align=\"center\">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</h1>\n",
        "  <p align=\"center\">\n",
        "    <a href=\"http://www.carldoersch.com/\">Carl Doersch</a>\n",
        "    ·\n",
        "    <a href=\"https://yangyi02.github.io/\">Yi Yang</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.com/citations?user=Jvi_XPAAAAAJ\">Mel Vecerik</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.com/citations?user=cnbENAEAAAAJ\">Dilara Gokay</a>\n",
        "    ·\n",
        "    <a href=\"https://www.robots.ox.ac.uk/~ankush/\">Ankush Gupta</a>\n",
        "    ·\n",
        "    <a href=\"http://people.csail.mit.edu/yusuf/\">Yusuf Aytar</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ\">Joao Carreira</a>\n",
        "    ·\n",
        "    <a href=\"https://www.robots.ox.ac.uk/~az/\">Andrew Zisserman</a>\n",
        "  </p>\n",
        "  <h3 align=\"center\"><a href=\"https://arxiv.org/abs/2306.08637\">Paper</a> | <a href=\"https://deepmind-tapir.github.io\">Project Page</a> | <a href=\"https://github.com/deepmind/tapnet\">GitHub</a> | <a href=\"https://github.com/deepmind/tapnet/tree/main#running-tapir-locally\">Live Demo</a> </h3>\n",
        "  <div align=\"center\"></div>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"\">\n",
        "    <img src=\"https://storage.googleapis.com/dm-tapnet/swaying_gif.gif\" alt=\"Logo\" width=\"50%\">\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCmDvfFvxnGB"
      },
      "outputs": [],
      "source": [
        "# @title Install code and dependencies {form-width: \"25%\"}\n",
        "!pip install 'tapnet[torch] @ git+https://github.com/google-deepmind/tapnet.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaswJZMq9B3c"
      },
      "outputs": [],
      "source": [
        "# @title Download Model {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/checkpoints\n",
        "\n",
        "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/bootstap/bootstapir_checkpoint_v2.pt\n",
        "\n",
        "%ls tapnet/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxlHY242m-6Q"
      },
      "outputs": [],
      "source": [
        "# @title Imports {form-width: \"25%\"}\n",
        "%matplotlib widget\n",
        "import haiku as hk\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import tree\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tapnet.torch import tapir_model\n",
        "from tapnet.utils import transforms\n",
        "from tapnet.utils import viz_utils\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sShqX4czvGuA"
      },
      "outputs": [],
      "source": [
        "# @title Select Device {form-width: \"25%\"}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogRTRVgfSq0W"
      },
      "outputs": [],
      "source": [
        "# @title Utility Functions {form-width: \"25%\"}\n",
        "\n",
        "def preprocess_frames(frames):\n",
        "  \"\"\"Preprocess frames to model inputs.\n",
        "\n",
        "  Args:\n",
        "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
        "\n",
        "  Returns:\n",
        "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
        "  \"\"\"\n",
        "  frames = frames.float()\n",
        "  frames = frames / 255 * 2 - 1\n",
        "  return frames\n",
        "\n",
        "\n",
        "def sample_random_points(frame_max_idx, height, width, num_points): # 随机采样 这些点的坐标顺序是（时间、高度、宽度），\n",
        "  # 其中时间t从 0 到frame_max_idx + 1随机采样，高度y和宽度x分别在给定的高度和宽度范围内随机采样。\n",
        "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
        "  y = np.random.randint(0, height, (num_points, 1)) # numpy.random.randint(low, high=None, size=None, dtype='l')\n",
        "  x = np.random.randint(0, width, (num_points, 1))\n",
        "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
        "  points = np.concatenate((t, y, x), axis=-1).astype(np.int32)  # [num_points, 3] 此时采样点的数据均为int\n",
        "  return points\n",
        "\n",
        "\n",
        "def postprocess_occlusions(occlusions, expected_dist):\n",
        "  visibles = (1 - F.sigmoid(occlusions)) * (1 - F.sigmoid(expected_dist)) > 0.5\n",
        "  return visibles\n",
        "\n",
        "\n",
        "def inference(frames, query_points, model):\n",
        "  # Preprocess video to match model inputs format\n",
        "  frames = preprocess_frames(frames)\n",
        "  num_frames, height, width = frames.shape[0:3]\n",
        "  query_points = query_points.float()\n",
        "  print(query_points[15:20])\n",
        "  frames, query_points = frames[None], query_points[None]\n",
        "\n",
        "  # Model inference\n",
        "  outputs = model(frames, query_points)\n",
        "  tracks, occlusions, expected_dist = outputs['tracks'][0], outputs['occlusion'][0], outputs['expected_dist'][0]\n",
        "\n",
        "  print(f\"tracks.shape:{outputs['tracks'].shape}\\n{outputs['tracks'][0]}\\n occlusion.shape:{outputs['occlusion'].shape}{outputs['occlusion'][0]}\\n expected_dist.shape:{outputs['expected_dist'].shape}{outputs['expected_dist'][0]}\")\n",
        "\n",
        "\n",
        "  # Binarize occlusions\n",
        "  visibles = postprocess_occlusions(occlusions, expected_dist)\n",
        "  return tracks, visibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HkuvyT6SJF"
      },
      "outputs": [],
      "source": [
        "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/examplar_videos\n",
        "\n",
        "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
        "\n",
        "video = media.read_video('tapnet/examplar_videos/horsejump-high.mp4')#[frames,height,weight,3]\n",
        "height, width = video.shape[1:3]\n",
        "media.show_video(video, fps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJxFMFM0vSeu"
      },
      "outputs": [],
      "source": [
        "# @title Define Model {form-width: \"25%\"}\n",
        "\n",
        "model = tapir_model.TAPIR(pyramid_level=1)\n",
        "model.load_state_dict(torch.load('tapnet/checkpoints/bootstapir_checkpoint_v2.pt'))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xN1TQIGfrDs"
      },
      "outputs": [],
      "source": [
        "# @title Set to Inference Mode to Save Memory {form-width: \"25%\"}\n",
        "\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIASK5A2Rj0X"
      },
      "outputs": [],
      "source": [
        "# @title Predict Sparse Point Tracks {form-width: \"25%\"}\n",
        "\n",
        "resize_height = 256  # @param {type: \"integer\"}\n",
        "resize_width = 256  # @param {type: \"integer\"}\n",
        "num_points = 50  # @param {type: \"integer\"}\n",
        "\n",
        "frames = media.resize_video(video, (resize_height, resize_width))\n",
        "query_points = sample_random_points(0, frames.shape[1], frames.shape[2], num_points)\n",
        "frames = torch.tensor(frames).to(device)\n",
        "query_points = torch.tensor(query_points).to(device)\n",
        "\n",
        "tracks, visibles = inference(frames, query_points, model)\n",
        "\n",
        "tracks = tracks.cpu().detach().numpy()\n",
        "visibles = visibles.cpu().detach().numpy()\n",
        "# Visualize sparse point tracks\n",
        "tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height)) # 转回原始resize前的尺寸空间\n",
        "video_viz = viz_utils.paint_point_track(video, tracks, visibles)# 在视频上绘制跟踪点\n",
        "media.show_video(video_viz, fps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY4hi_uTv6K4"
      },
      "outputs": [],
      "source": [
        "# @title Select Any Points at Any Frame {form-width: \"25%\"}\n",
        "\n",
        "select_frame = 7  #@param {type:\"slider\", min:0, max:49, step:1}\n",
        "\n",
        "# Generate a colormap with 20 points, no need to change unless select more than 20 points\n",
        "colormap = viz_utils.get_colors(20)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.imshow(video[select_frame])\n",
        "ax.axis('off')\n",
        "ax.set_title('You can select more than 1 points. After select enough points, run the next cell.')\n",
        "\n",
        "select_points = []\n",
        "\n",
        " # Event handler for mouse clicks\n",
        "def on_click(event):\n",
        "  if event.button == 1 and event.inaxes == ax:  # Left mouse button clicked\n",
        "    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
        "\n",
        "    select_points.append(np.array([x, y]))\n",
        "\n",
        "    color = colormap[len(select_points) - 1]\n",
        "    color = tuple(np.array(color) / 255.0)\n",
        "    ax.plot(x, y, 'o', color=color, markersize=5)\n",
        "    plt.draw()\n",
        "\n",
        "fig.canvas.mpl_connect('button_press_event', on_click)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-JUf0pfwFN2"
      },
      "outputs": [],
      "source": [
        "# @title Predict Point Tracks for the Selected Points {form-width: \"25%\"}\n",
        "\n",
        "resize_height = 256  # @param {type: \"integer\"}\n",
        "resize_width = 256  # @param {type: \"integer\"}\n",
        "\n",
        "def convert_select_points_to_query_points(frame, points):\n",
        "  \"\"\"Convert select points to query points.\n",
        "\n",
        "  Args:\n",
        "    points: [num_points, 2], in [x, y]\n",
        "  Returns:\n",
        "    query_points: [num_points, 3], in [t, y, x]\n",
        "  \"\"\"\n",
        "  points = np.stack(points)\n",
        "  query_points = np.zeros(shape=(points.shape[0], 3), dtype=np.float32)\n",
        "  query_points[:, 0] = frame\n",
        "  query_points[:, 1] = points[:, 1]\n",
        "  query_points[:, 2] = points[:, 0]\n",
        "  return query_points\n",
        "\n",
        "frames = media.resize_video(video, (resize_height, resize_width))\n",
        "query_points = convert_select_points_to_query_points(select_frame, select_points)\n",
        "query_points = transforms.convert_grid_coordinates(\n",
        "    query_points, (1, height, width), (1, resize_height, resize_width), coordinate_format='tyx')\n",
        "\n",
        "frames = torch.tensor(frames).to(device)\n",
        "query_points = torch.tensor(query_points).to(device)\n",
        "tracks, visibles = inference(frames, query_points, model)\n",
        "\n",
        "tracks = tracks.detach().cpu().numpy()\n",
        "visibles = visibles.detach().cpu().numpy()\n",
        "\n",
        "# Visualize sparse point tracks\n",
        "tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height))\n",
        "video_viz = viz_utils.paint_point_track(video, tracks, visibles, colormap)\n",
        "media.show_video(video_viz, fps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8orfQRoaRJit"
      },
      "source": [
        "That's it!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}